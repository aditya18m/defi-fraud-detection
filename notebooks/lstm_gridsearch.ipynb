{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa1223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    classification_report, precision_recall_curve, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    accuracy_score, precision_score, recall_score, fbeta_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b738b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed/lstm_sequences_alemari_normalised.csv\") \n",
    "X = df.drop(\"label\", axis=1).values.astype(np.float32)\n",
    "y = df[\"label\"].values.astype(np.int64)\n",
    "X = X.reshape((X.shape[0], 5, -1))  # (N, seq_len=5, input_size=4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a1b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        out = self.fc(h_n[-1])\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8c1757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_lstm_fp_exact(X, y, m_grid, arch_grid, fp_exact=64, beta=2.0):#change fp exact -> 64 for 1%, 32 for 0.5%\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx, test_idx = next(split.split(X, y))\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "    torch.save(test_idx, \"../configs/lstm_test_mask_4feat_exact64.pt\")\n",
    "\n",
    "    best_result = None\n",
    "    best_f2 = -1\n",
    "    closest_result = None\n",
    "    min_fp_diff = float(\"inf\")\n",
    "\n",
    "    for arch in arch_grid:\n",
    "        print(f\"\\n Trying arch: {arch}\")\n",
    "        for m in m_grid:\n",
    "            model = LSTMClassifier(input_size=X.shape[2],\n",
    "                                   hidden_size=arch[\"hidden_dim\"],\n",
    "                                   num_layers=arch[\"layers\"],\n",
    "                                   dropout=arch[\"dropout\"] if arch[\"layers\"] > 1 else 0.0).to(device)\n",
    "\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=arch[\"lr\"])\n",
    "            class_weights = torch.tensor([1.0, m], dtype=torch.float32).to(device)\n",
    "            criterion = nn.NLLLoss(weight=class_weights)\n",
    "\n",
    "            X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "            y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "\n",
    "            for epoch in range(100):\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                out = model(X_train_tensor)\n",
    "                loss = criterion(out, y_train_tensor)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "                logits = model(X_test_tensor).cpu()\n",
    "                probs = torch.exp(logits)[:, 1].numpy()\n",
    "\n",
    "            labels = y_test\n",
    "            thresholds = np.linspace(0.01, 0.99, 500)\n",
    "            for t in thresholds:\n",
    "                preds = (probs > t).astype(int)\n",
    "                fp = ((labels == 0) & (preds == 1)).sum()\n",
    "                tp = ((labels == 1) & (preds == 1)).sum()\n",
    "                fn = ((labels == 1) & (preds == 0)).sum()\n",
    "                prec = tp / (tp + fp + 1e-8)\n",
    "                rec = tp / (tp + fn + 1e-8)\n",
    "                fbeta = (1 + beta**2) * prec * rec / (beta**2 * prec + rec + 1e-8)\n",
    "\n",
    "                if abs(fp - fp_exact) <= 1 and fbeta > best_f2:\n",
    "                    best_f2 = fbeta\n",
    "                    best_result = {\n",
    "                        \"m\": float(m),\n",
    "                        \"thr\": float(t),\n",
    "                        \"Fbeta\": float(fbeta),\n",
    "                        \"precision\": float(prec),\n",
    "                        \"recall\": float(rec),\n",
    "                        \"false_positives\": int(fp),\n",
    "                        \"state_dict\": model.state_dict(),\n",
    "                        \"arch\": arch\n",
    "                    }\n",
    "\n",
    "                if abs(fp - fp_exact) < min_fp_diff or (abs(fp - fp_exact) == min_fp_diff and fbeta > (closest_result or {}).get(\"Fbeta\", 0)):\n",
    "                    min_fp_diff = abs(fp - fp_exact)\n",
    "                    closest_result = {\n",
    "                        \"m\": float(m),\n",
    "                        \"thr\": float(t),\n",
    "                        \"Fbeta\": float(fbeta),\n",
    "                        \"precision\": float(prec),\n",
    "                        \"recall\": float(rec),\n",
    "                        \"false_positives\": int(fp),\n",
    "                        \"state_dict\": model.state_dict(),\n",
    "                        \"arch\": arch\n",
    "                    }\n",
    "\n",
    "    result_to_use = best_result if best_result else closest_result\n",
    "\n",
    "    if result_to_use:\n",
    "        fname = \"../models/best_lstm_model_4feat_exact64.pth\"\n",
    "        torch.save(result_to_use[\"state_dict\"], fname)\n",
    "        with open(\"../configs/lstm_best_config_4feat_exact64.json\", \"w\") as f:\n",
    "            json.dump({k: v for k, v in result_to_use.items() if k != \"state_dict\"}, f, indent=2)\n",
    "        print(f\"\\Saved best LSTM model (FP â‰ˆ {fp_exact}, F2 = {result_to_use['Fbeta']:.4f})\")\n",
    "        return result_to_use\n",
    "    else:\n",
    "        raise RuntimeError(\"No viable config found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c7af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_grid = [\n",
    "    {\"hidden_dim\": 64,  \"lr\": 0.001,  \"layers\": 2, \"dropout\": 0.2},\n",
    "    {\"hidden_dim\": 128, \"lr\": 0.001,  \"layers\": 3, \"dropout\": 0.3},\n",
    "    {\"hidden_dim\": 32,  \"lr\": 0.0005, \"layers\": 2, \"dropout\": 0.1},\n",
    "    {\"hidden_dim\": 64,  \"lr\": 0.0005, \"layers\": 1, \"dropout\": 0.0},\n",
    "    {\"hidden_dim\": 128, \"lr\": 0.001,  \"layers\": 2, \"dropout\": 0.1}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa7b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_grid = np.round(np.arange(1.0, 4.0, 0.1), 2)\n",
    "#run the sweep\n",
    "best_result = grid_search_lstm_fp_exact(X, y, m_grid, arch_grid, fp_exact=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee2fb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test indices\n",
    "test_idx = torch.load(\"../configs/lstm_test_mask_exact64.pt\")\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "arch = best_result[\"arch\"]\n",
    "model = LSTMClassifier(input_size=X.shape[2],\n",
    "                       hidden_size=arch[\"hidden_dim\"],\n",
    "                       num_layers=arch[\"layers\"],\n",
    "                       dropout=arch[\"dropout\"] if arch[\"layers\"] > 1 else 0.0).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"../models/best_lstm_model_exact64.pth\"))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test_tensor).cpu()\n",
    "    probs = torch.exp(logits)[:, 1].numpy()\n",
    "\n",
    "#threshold and prediction\n",
    "threshold = best_result[\"thr\"]\n",
    "preds = (probs > threshold).astype(int)\n",
    "labels = y_test\n",
    "\n",
    "#Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(labels, preds, target_names=[\"Legit\", \"Phishing\"]))\n",
    "\n",
    "#F2-score\n",
    "f2 = fbeta_score(labels, preds, beta=2.0)\n",
    "print(f\"\\nF2-Score (beta=2.0): {f2:.4f}\")\n",
    "\n",
    "#confusion Matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Legit\", \"Phishing\"],\n",
    "            yticklabels=[\"Legit\", \"Phishing\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix (Threshold = {:.4f})\".format(threshold))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(labels, probs)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(rec, prec, label=\"PR Curve\")\n",
    "plt.scatter(best_result[\"recall\"], best_result[\"precision\"], color='red',\n",
    "            label=f\"Best Threshold = {threshold:.4f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
