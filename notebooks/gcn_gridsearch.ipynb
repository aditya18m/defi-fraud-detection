{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b32dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from sklearn.metrics import fbeta_score, classification_report, confusion_matrix, precision_recall_curve\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636adef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(\"../data/graphs/alemari_graph.pt\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba01ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout=0.5, use_batchnorm=False):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList() if use_batchnorm else None\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.layers.append(GCNConv(in_channels, hidden_channels))\n",
    "        if use_batchnorm:\n",
    "            self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GCNConv(hidden_channels, hidden_channels))\n",
    "            if use_batchnorm:\n",
    "                self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.layers.append(GCNConv(hidden_channels, out_channels))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            x = layer(x, edge_index)\n",
    "            if self.use_batchnorm:\n",
    "                x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        return F.log_softmax(self.layers[-1](x, edge_index), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4867f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_gcn(data, m_grid, fp_exact=146, beta=2.0): #change fp exact -> 146 for 1%, 73 for 0.5%\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx, test_idx = next(split.split(torch.arange(len(data.y)), data.y.cpu()))\n",
    "    train_mask = torch.tensor(train_idx, dtype=torch.long, device=device)\n",
    "    test_mask = torch.tensor(test_idx, dtype=torch.long, device=device)\n",
    "    torch.save(test_mask, \"../configs/gcn_test_mask_4feat_146fp.pt\")\n",
    "\n",
    "    best_result = None\n",
    "    best_f2 = -1\n",
    "\n",
    "    arch_grid = [\n",
    "        {\"hidden_dim\": 128, \"lr\": 0.001,  \"weight_decay\": 5e-4, \"layers\": 2, \"dropout\": 0.5, \"bn\": False},\n",
    "        {\"hidden_dim\": 128, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"layers\": 3, \"dropout\": 0.3, \"bn\": True},\n",
    "        {\"hidden_dim\": 64,  \"lr\": 0.001,  \"weight_decay\": 1e-4, \"layers\": 2, \"dropout\": 0.2, \"bn\": False},\n",
    "        {\"hidden_dim\": 128, \"lr\": 0.001,  \"weight_decay\": 1e-3, \"layers\": 4, \"dropout\": 0.6, \"bn\": True},\n",
    "        {\"hidden_dim\": 256, \"lr\": 0.0005, \"weight_decay\": 1e-4, \"layers\": 3, \"dropout\": 0.4, \"bn\": True},\n",
    "        {\"hidden_dim\": 128, \"lr\": 0.001,  \"weight_decay\": 0.0,  \"layers\": 3, \"dropout\": 0.0, \"bn\": False},\n",
    "        {\"hidden_dim\": 64,  \"lr\": 0.001,  \"weight_decay\": 5e-4, \"layers\": 2, \"dropout\": 0.3, \"bn\": True},\n",
    "        {\"hidden_dim\": 128, \"lr\": 0.0005, \"weight_decay\": 1e-3, \"layers\": 5, \"dropout\": 0.5, \"bn\": True}\n",
    "    ]\n",
    "\n",
    "    for arch in arch_grid:\n",
    "        for m in m_grid:\n",
    "            model = GCN(\n",
    "                in_channels=data.num_features,\n",
    "                hidden_channels=arch[\"hidden_dim\"],\n",
    "                out_channels=2,\n",
    "                num_layers=arch[\"layers\"],\n",
    "                dropout=arch[\"dropout\"],\n",
    "                use_batchnorm=arch[\"bn\"]\n",
    "            ).to(device)\n",
    "\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=arch[\"lr\"], weight_decay=arch[\"weight_decay\"])\n",
    "            class_weights = 1.0 / torch.bincount(data.y).float()\n",
    "            class_weights[1] *= m\n",
    "            criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "            for _ in range(200):\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                out = model(data)\n",
    "                loss = criterion(out[train_mask], data.y[train_mask])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                logits = model(data)[test_mask]\n",
    "                probs = torch.exp(logits)[:, 1].cpu().numpy()\n",
    "                labels = data.y[test_mask].cpu().numpy()\n",
    "\n",
    "            prec, rec, thr = precision_recall_curve(labels, probs)\n",
    "            f2_arr = (1 + beta**2) * prec * rec / (beta**2 * prec + rec + 1e-8)\n",
    "\n",
    "            for i in range(len(thr)):\n",
    "                if thr[i] < 1e-6:\n",
    "                    continue\n",
    "                preds = (probs > thr[i]).astype(int)\n",
    "                fp = ((labels == 0) & (preds == 1)).sum()\n",
    "                if fp == fp_exact and f2_arr[i] > best_f2:\n",
    "                    best_f2 = f2_arr[i]\n",
    "                    best_result = {\n",
    "                        \"m\": float(m),\n",
    "                        \"thr\": float(thr[i]),\n",
    "                        \"Fbeta\": float(f2_arr[i]),\n",
    "                        \"precision\": float(prec[i]),\n",
    "                        \"recall\": float(rec[i]),\n",
    "                        \"false_positives\": int(fp),\n",
    "                        \"state_dict\": model.state_dict(),\n",
    "                        \"arch\": arch\n",
    "                    }\n",
    "\n",
    "    if best_result:\n",
    "        torch.save(best_result[\"state_dict\"], \"../models/best_gcn_model_4feat_146fp.pth\")\n",
    "        with open(\"../configs/gcn_best_config_4feat_146fp.json\", \"w\") as f:\n",
    "            json.dump({k: v for k, v in best_result.items() if k != \"state_dict\"}, f, indent=2)\n",
    "        print(f\"Saved best GCN model with 4 features (FP={fp_exact}, F2={best_f2:.4f})\")\n",
    "        return best_result\n",
    "    else:\n",
    "        raise ValueError(\"No GCN configuration met the FP constraint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4b37b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run grid sweep\n",
    "m_grid = np.round(np.arange(1.0, 4.0, 0.1), 2)\n",
    "best_result = grid_search_gcn(data, m_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a03cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = best_result[\"arch\"]\n",
    "model = GCNConv(\n",
    "    in_channels=data.num_features,\n",
    "    hidden_channels=arch[\"hidden_dim\"],\n",
    "    out_channels=2,\n",
    "    num_layers=arch[\"layers\"],\n",
    "    dropout=arch[\"dropout\"],\n",
    "    use_batchnorm=arch[\"bn\"]\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(best_result[\"state_dict\"])\n",
    "model.eval()\n",
    "test_mask = torch.load(\"../configs/gcn_test_mask_exact146.pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(data)[test_mask]\n",
    "    probs = torch.exp(logits)[:, 1].cpu().numpy()\n",
    "    labels = data.y[test_mask].cpu().numpy()\n",
    "\n",
    "#apply best threshold\n",
    "threshold = best_result[\"thr\"]\n",
    "preds = (probs > threshold).astype(int)\n",
    "\n",
    "#Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(labels, preds, target_names=[\"Legit\", \"Phishing\"]))\n",
    "#compute F2-score\n",
    "f2 = fbeta_score(labels, preds, beta=2.0)\n",
    "print(f\"\\nF2-Score (beta=2.0): {f2:.4f}\")\n",
    "\n",
    "\n",
    "#Confusion Matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Legit\", \"Phishing\"], yticklabels=[\"Legit\", \"Phishing\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix (Threshold = {:.4f})\".format(threshold))\n",
    "plt.show()\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(labels, probs)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(rec, prec, label=\"PR Curve\")\n",
    "plt.scatter(best_result[\"recall\"], best_result[\"precision\"], color='red', label=\"Best Threshold = {:.4f}\".format(threshold))\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
